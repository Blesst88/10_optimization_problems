{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blesst88/10_optimization_problems/blob/master/blog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj5huAQ9MU92",
        "outputId": "32d4d071-1313-4a21-8ebd-8143343940ef"
      },
      "source": [
        "!pip install torch==1.4.0 torchvision==0.5.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/19/4804aea17cd136f1705a5e98a00618cb8f6ccc375ad8bfa437408e09d058/torch-1.4.0-cp36-cp36m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 21kB/s \n",
            "\u001b[?25hCollecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/90/6141bf41f5655c78e24f40f710fdd4f8a8aff6c8b7c6f0328240f649bdbe/torchvision-0.5.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 52.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed torch-1.4.0 torchvision-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_JjVd5tNi6V",
        "outputId": "20e741e9-a4ff-4a32-e462-35a7f2507dac"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OQD_bSRkXBp"
      },
      "source": [
        "!mkdir input\n",
        "!mkdir outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZFOUghojR7u",
        "outputId": "266e3bce-853a-418e-82b7-9c669d744956"
      },
      "source": [
        "%%writefile dcgan.py\n",
        "import torch.nn as nn\n",
        "\n",
        "# generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nz = nz\n",
        "        self.main = nn.Sequential(\n",
        "            # nz will be the input to the first convolution\n",
        "            nn.ConvTranspose2d(\n",
        "                nz, 512, kernel_size=4, \n",
        "                stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(\n",
        "                512, 256, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(\n",
        "                256, 128, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(\n",
        "                128, 64, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(\n",
        "                64, 3, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "# discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                3, 64, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(\n",
        "                64, 128, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(\n",
        "                128, 256, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(\n",
        "                256, 512, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(\n",
        "                512, 1, kernel_size=4, \n",
        "                stride=1, padding=0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing dcgan.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-YIJHRCjmb6",
        "outputId": "5989a698-32d2-4274-a8ff-91fb0f689bc0"
      },
      "source": [
        "%%writefile utils.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# set the computation device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def label_real(size):\n",
        "    \"\"\"\n",
        "    Fucntion to create real labels (ones)\n",
        "    :param size: batch size\n",
        "    :return real label vector\n",
        "    \"\"\"\n",
        "    data = torch.ones(size, 1)\n",
        "    return data.to(device)\n",
        "\n",
        "def label_fake(size):\n",
        "    \"\"\"\n",
        "    Fucntion to create fake labels (zeros)\n",
        "    :param size: batch size\n",
        "    :returns fake label vector\n",
        "    \"\"\"\n",
        "    data = torch.zeros(size, 1)\n",
        "    return data.to(device)\n",
        "\n",
        "def create_noise(sample_size, nz):\n",
        "    \"\"\"\n",
        "    Fucntion to create noise\n",
        "    :param sample_size: fixed sample size or batch size\n",
        "    :param nz: latent vector size\n",
        "    :returns random noise vector\n",
        "    \"\"\"\n",
        "    return torch.randn(sample_size, nz, 1, 1).to(device)\n",
        "\n",
        "def save_generator_image(image, path):\n",
        "    \"\"\"\n",
        "    Function to save torch image batches\n",
        "    :param image: image tensor batch\n",
        "    :param path: path name to save image\n",
        "    \"\"\"\n",
        "    save_image(image, path, normalize=True)\n",
        "\n",
        "def weights_init(m):\n",
        "    \"\"\"\n",
        "    This function initializes the model weights randomly from a \n",
        "    Normal distribution. This follows the specification from the DCGAN paper.\n",
        "    https://arxiv.org/pdf/1511.06434.pdf\n",
        "    Source: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK8Xmc5tjyTo",
        "outputId": "f83f5083-0501-4c37-d76a-201856717b22"
      },
      "source": [
        "%%writefile train_dcgan.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "from utils import save_generator_image, weights_init\n",
        "from utils import label_fake, label_real, create_noise\n",
        "from dcgan import Generator, Discriminator\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "# learning parameters / configurations according to paper\n",
        "image_size = 64 # we need to resize image to 64x64\n",
        "batch_size = 128\n",
        "nz = 100 # latent vector size\n",
        "beta1 = 0.5 # beta1 value for Adam optimizer\n",
        "lr = 0.0002 # learning rate according to paper\n",
        "sample_size = 64 # fixed sample size\n",
        "epochs = 25 # number of epoch to train\n",
        "\n",
        "# set the computation device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# image transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), \n",
        "    (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# prepare the data\n",
        "train_data = datasets.CIFAR10(\n",
        "    root='input/data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# initialize models\n",
        "generator = Generator(nz).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# initialize generator weights\n",
        "generator.apply(weights_init)\n",
        "# initialize discriminator weights\n",
        "discriminator.apply(weights_init)\n",
        "\n",
        "print('##### GENERATOR #####')\n",
        "print(generator)\n",
        "print('######################')\n",
        "\n",
        "print('\\n##### DISCRIMINATOR #####')\n",
        "print(discriminator)\n",
        "print('######################')\n",
        "\n",
        "# optimizers\n",
        "optim_g = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optim_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "losses_g = [] # to store generator loss after each epoch\n",
        "losses_d = [] # to store discriminator loss after each epoch\n",
        "\n",
        "# function to train the discriminator network\n",
        "def train_discriminator(optimizer, data_real, data_fake):\n",
        "    b_size = data_real.size(0)\n",
        "    # get the real label vector\n",
        "    real_label = label_real(b_size)\n",
        "    # get the fake label vector\n",
        "    fake_label = label_fake(b_size)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get the outputs by doing real data forward pass\n",
        "    output_real = discriminator(data_real).view(-1)\n",
        "    loss_real = criterion(output_real, real_label)\n",
        "\n",
        "    # get the outputs by doing fake data forward pass\n",
        "    output_fake = discriminator(data_fake)\n",
        "    loss_fake = criterion(output_fake, fake_label)\n",
        "\n",
        "    # compute gradients of real loss \n",
        "    loss_real.backward()\n",
        "    # compute gradients of fake loss\n",
        "    loss_fake.backward()\n",
        "    # update discriminator parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss_real + loss_fake\n",
        "\n",
        "# function to train the generator network\n",
        "def train_generator(optimizer, data_fake):\n",
        "    b_size = data_fake.size(0)\n",
        "    # get the real label vector\n",
        "    real_label = label_real(b_size)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # output by doing a forward pass of the fake data through discriminator\n",
        "    output = discriminator(data_fake)\n",
        "    loss = criterion(output, real_label)\n",
        "\n",
        "    # compute gradients of loss\n",
        "    loss.backward()\n",
        "    # update generator parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss    \n",
        "\n",
        "# create the noise vector\n",
        "noise = create_noise(sample_size, nz)\n",
        "# print('SIZE', noise.size())\n",
        "# print('NOISE', noise)\n",
        "\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_g = 0.0\n",
        "    loss_d = 0.0\n",
        "    for bi, data in tqdm(enumerate(train_loader), total=int(len(train_data)/train_loader.batch_size)):\n",
        "        image, _ = data\n",
        "        image = image.to(device)\n",
        "        b_size = len(image)\n",
        "        # forward pass through generator to create fake data\n",
        "        data_fake = generator(create_noise(b_size, nz)).detach()\n",
        "        data_real = image\n",
        "        loss_d += train_discriminator(optim_d, data_real, data_fake)\n",
        "        data_fake = generator(create_noise(b_size, nz))\n",
        "        loss_g += train_generator(optim_g, data_fake)\n",
        "\n",
        "    # final forward pass through generator to create fake data...\n",
        "    # ...after training for current epoch\n",
        "    generated_img = generator(noise).cpu().detach()\n",
        "    # save the generated torch tensor models to disk\n",
        "    save_generator_image(generated_img, f\"outputs/gen_img{epoch}.png\")\n",
        "    epoch_loss_g = loss_g / bi # total generator loss for the epoch\n",
        "    epoch_loss_d = loss_d / bi # total discriminator loss for the epoch\n",
        "    losses_g.append(epoch_loss_g)\n",
        "    losses_d.append(epoch_loss_d)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "    print(f\"Generator loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}\")\n",
        "\n",
        "print('DONE TRAINING')\n",
        "# save the model weights to disk\n",
        "torch.save(generator.state_dict(), 'outputs/generator.pth')\n",
        "\n",
        "# plot and save the generator and discriminator loss\n",
        "plt.figure()\n",
        "plt.plot(losses_g, label='Generator loss')\n",
        "plt.plot(losses_d, label='Discriminator Loss')\n",
        "plt.legend()\n",
        "plt.savefig('outputs/loss.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing train_dcgan.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkfJL5ZqkQbG",
        "outputId": "f8ce5271-c35f-44e3-f639-3bfa9597368e"
      },
      "source": [
        "!python train_dcgan.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to input/data/cifar-10-python.tar.gz\n",
            "170500096it [00:01, 96063961.21it/s]                   \n",
            "Extracting input/data/cifar-10-python.tar.gz to input/data\n",
            "##### GENERATOR #####\n",
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n",
            "######################\n",
            "\n",
            "##### DISCRIMINATOR #####\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n",
            "######################\n",
            "  0% 0/390 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "100% 390/390 [01:47<00:00,  3.55it/s]/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([80, 1])) that is different to the input size (torch.Size([80])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([80, 1])) that is different to the input size (torch.Size([80, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "391it [01:47,  3.64it/s]\n",
            "Epoch 1 of 25\n",
            "Generator loss: 12.18607140, Discriminator loss: 0.49181098\n",
            " 10% 40/390 [00:11<01:39,  3.53it/s]Traceback (most recent call last):\n",
            "  File \"train_dcgan.py\", line 138, in <module>\n",
            "    loss_d += train_discriminator(optim_d, data_real, data_fake)\n",
            "  File \"train_dcgan.py\", line 95, in train_discriminator\n",
            "    loss_fake.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 195, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 99, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n",
            " 10% 40/390 [00:11<01:40,  3.50it/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fl1cG1Tkg0l"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}